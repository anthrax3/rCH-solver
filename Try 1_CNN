# source: https://github.com/fchollet/keras/blob/master/examples/mnist_mlp.py

import numpy as np
from keras.datasets import mnist # data set
from keras.models import Sequential # layer setup
from keras.layers import Dense, Activation, Dropout # type of n/w
from keras.optimizers import SGD, Adam, RMSprop # type of GD
from keras.utils import np_utils
import matplotlib.pyplot as plt
%matplotlib inline

batch_size = 10
nb_classes = 10
nb_epoch = 10

# the data, shuffled and split between train and test sets
(X_train, y_train), (X_test, y_test) = mnist.load_data()

plt.pcolor(X_train[6].reshape(28,28)) # check out the contents

X_train = X_train.reshape(60000, 784)
X_test = X_test.reshape(10000, 784)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')
X_train /= 255
X_test /= 255
print(X_train.shape[0], 'train samples')
print(X_test.shape[0], 'test samples')

#convert the numbers into binary equivalent
Y_train = np_utils.to_categorical(y_train, nb_classes) 
Y_test = np_utils.to_categorical(y_test, nb_classes)
    
model = Sequential()
model.add(Dense(128,input_shape =(784,)  ))
model.add(Activation('sigmoid'))
model.add(Dense(10))
model.add(Activation('softmax'))
rms = RMSprop()
model.summary()
% time model.compile(loss = "categorical_crossentropy",optimizer = rms)


model.summary()
model.fit(X_train,Y_train, batch_size=batch_size, nb_epoch=nb_epoch, validation_data=(X_test,Y_test),verbose=1)